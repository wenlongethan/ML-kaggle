import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
import os
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# 1. å‡†å¤‡æ•°æ®
# ==========================================
base_path = '/Users/liangwenlong/study/bme/3/ml_usage/project/truck-fuel-consumption-forecast'
train_path = os.path.join(base_path, 'public_train.csv')
test_path = os.path.join(base_path, 'public_test.csv')

print("æ­£åœ¨è¯»å–æ•°æ®...")
train = pd.read_csv(train_path)
test = pd.read_csv(test_path)

target_col = 'fuel_consumption_sum'

# ==========================================
# 2. ç‰¹å¾å·¥ç¨‹ V2 (å¢å¼ºç‰ˆ)
# ==========================================
print("æ­£åœ¨è¿›è¡Œç‰¹å¾å·¥ç¨‹ V2...")


def engineer_features(df):
    # --- ç‰©ç†ç‰¹å¾ (ä¿ç•™ V1 çš„ç²¾å) ---
    # åŠ¨èƒ½: 0.5 * m * v^2
    df['kinetic_energy'] = df['weight_1'] * (df['speed_mean'] ** 2)
    # åŠ¨é‡: m * v
    df['momentum'] = df['weight_1'] * df['speed_mean']

    # --- é©¾é©¶è¡Œä¸º ---
    # åˆ¹è½¦ä¸æ²¹é—¨äº¤äº’
    df['braking_intensity'] = df['brake_switch_mean'] * df['speed_mean']
    df['pedal_vs_speed'] = df['accelerator_pedal_position_mean'] * df['engine_speed_mean']

    # --- ç¯å¢ƒ ---
    # é¡ºé£/é€†é£ç³»æ•° (å‡è®¾ sailing_value æ˜¯æ­£å‘çš„)
    if 'env_sailing_value' in df.columns:
        df['wind_assist'] = df['env_wind_kph'] * df['env_sailing_value']

    # --- [æ–°å¢] äº¤äº’ç‰¹å¾ ---
    # è´Ÿè½½ä¸å¡åº¦ (å¦‚æœæœ‰ road_level)
    if 'road_level_approximation' in df.columns:
        df['load_on_slope'] = df['weight_1'] * df['road_level_approximation']

    return df


train = engineer_features(train)
test = engineer_features(test)

# ==========================================
# 3. æ•°æ®æ¸…æ´—ä¸ç±»åˆ«å¤„ç† (å…³é”®ä¿®å¤)
# ==========================================
# å‰”é™¤ä¸éœ€è¦çš„åˆ—
drop_cols = ['ID', 'Trip_ID_first', 'Trip_ID_last', target_col]
features = [c for c in train.columns if c not in drop_cols]

# --- å¼ºåˆ¶æŒ‡å®šç±»åˆ«ç‰¹å¾ ---
# å“ªæ€•å®ƒä»¬çœ‹èµ·æ¥åƒæ•°å­—ï¼Œåªè¦ä»£è¡¨IDæˆ–ç±»å‹ï¼Œå°±è½¬ä¸º category
potential_cats = ['vehicle_type', 'vehicle_motortype', 'driver_name_and_id',
                  'route_id', 'deviceuniquecode']

cat_cols = []
for col in features:
    # å¦‚æœåˆ—ååŒ…å« id, type, code æˆ–è€…æœ¬èº«å°±æ˜¯ object ç±»å‹
    if train[col].dtype == 'object' or any(x in col.lower() for x in ['type', 'id', 'code', 'name']):
        # ç¡®ä¿åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½å­˜åœ¨
        train[col] = train[col].astype('category')
        test[col] = test[col].astype('category')
        cat_cols.append(col)

print(f"ä½¿ç”¨çš„ç‰¹å¾æ•°é‡: {len(features)}")
print(f"ğŸ”¥ è¯†åˆ«åˆ°çš„ç±»åˆ«ç‰¹å¾ (å°†è¢«ç‰¹æ®Šå¤„ç†): {cat_cols}")

# ==========================================
# 4. LightGBM è®­ç»ƒ (Log å˜æ¢ + æ›´å¤šè½®æ•°)
# ==========================================
folds = 5
kf = KFold(n_splits=folds, shuffle=True, random_state=42)

# !!! æ ¸å¿ƒæŠ€å·§ï¼šå¯¹ç›®æ ‡å˜é‡å– Logï¼Œè®©åˆ†å¸ƒæ›´æ­£æ€ !!!
# é¢„æµ‹å®Œæˆåå†ç”¨ exp è¿˜åŸ
y_target = np.log1p(train[target_col])

oof_preds_log = np.zeros(len(train))
test_preds_log = np.zeros(len(test))
scores = []
feature_importance_df = pd.DataFrame()

print(f"å¼€å§‹è®­ç»ƒ LightGBM (CV={folds}, Max Rounds=10000)...")

for fold, (train_idx, val_idx) in enumerate(kf.split(train)):
    X_train, y_train = train[features].iloc[train_idx], y_target.iloc[train_idx]
    X_val, y_val = train[features].iloc[val_idx], y_target.iloc[val_idx]

    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'learning_rate': 0.02,  # ç¨å¾®é™ä½å­¦ä¹ ç‡ï¼Œé€šè¿‡å¢åŠ è½®æ•°æ¥æå‡ç²¾åº¦
        'num_leaves': 64,  # å¢åŠ æ ‘çš„å¤æ‚åº¦
        'feature_fraction': 0.8,
        'bagging_fraction': 0.7,
        'bagging_freq': 1,
        'verbose': -1,
        'n_jobs': -1
    }

    model = lgb.train(
        params,
        lgb.Dataset(X_train, y_train, categorical_feature=cat_cols),
        num_boost_round=10000,  # !!! å¤§å¹…å¢åŠ ä¸Šé™ !!!
        valid_sets=[lgb.Dataset(X_val, y_val)],
        callbacks=[lgb.early_stopping(300), lgb.log_evaluation(1000)]
    )

    # é¢„æµ‹å¹¶è¿˜åŸ (exp)
    val_pred_log = model.predict(X_val)
    oof_preds_log[val_idx] = val_pred_log
    test_preds_log += model.predict(test[features]) / folds

    # è¿˜åŸåˆ°åŸå§‹å°ºåº¦è®¡ç®— RMSE
    val_pred_original = np.expm1(val_pred_log)
    y_val_original = np.expm1(y_val)

    rmse = np.sqrt(mean_squared_error(y_val_original, val_pred_original))
    scores.append(rmse)
    print(f"Fold {fold + 1} RMSE: {rmse:.4f}")

    # è®°å½•é‡è¦æ€§
    fold_importance = pd.DataFrame()
    fold_importance["feature"] = features
    fold_importance["importance"] = model.feature_importance()
    fold_importance["fold"] = fold + 1
    feature_importance_df = pd.concat([feature_importance_df, fold_importance], axis=0)

# ==========================================
# 5. ç»“æœä¸ç»˜å›¾
# ==========================================
mean_rmse = np.mean(scores)
print(f"\n========================================")
print(f"ğŸ”¥ V2ç‰ˆæœ¬ å¹³å‡ RMSE: {mean_rmse:.4f}")
print(f"========================================")

# ç”Ÿæˆæäº¤
final_preds = np.expm1(test_preds_log)  # è®°å¾—è¿˜åŸ
submission = pd.DataFrame({'ID': test['ID'], target_col: final_preds})
sub_filename = f'submission_v2_log_rmse_{mean_rmse:.4f}.csv'
submission.to_csv(sub_filename, index=False)
print(f"æäº¤æ–‡ä»¶å·²ç”Ÿæˆ: {sub_filename}")

# ç»˜å›¾
plt.figure(figsize=(10, 8))
cols = (feature_importance_df[["feature", "importance"]]
        .groupby("feature")
        .mean()
        .sort_values(by="importance", ascending=False)[:20].index)
best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]
sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False))
plt.title('Top 20 Features (V2 Model)')
plt.tight_layout()
plt.savefig('feature_importance_v2.png')