import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
import os
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# 1. å‡†å¤‡æ•°æ®
# ==========================================
base_path = '/Users/liangwenlong/study/bme/3/ml_usage/project/truck-fuel-consumption-forecast'
train_path = os.path.join(base_path, 'public_train.csv')
test_path = os.path.join(base_path, 'public_test.csv')

print("æ­£åœ¨è¯»å–æ•°æ®...")
train = pd.read_csv(train_path)
test = pd.read_csv(test_path)

target_col = 'fuel_consumption_sum'


# ==========================================
# 2. æ ¸å¿ƒä¿®å¤ï¼šæ•°æ®ç±»å‹æ¸…æ´— (Fix Types)
# ==========================================
def clean_numeric_columns(df):
    # å¼ºåˆ¶è½¬æ¢è¿™äº›çœ‹ä¼¼æ˜¯å­—ç¬¦ä¸²å…¶å®æ˜¯æ•°å­—çš„åˆ—
    # åªè¦åˆ—åé‡ŒåŒ…å« humidity, wind, temp, speed ç­‰ï¼Œéƒ½å°è¯•è½¬æ•°å­—
    for col in df.columns:
        if df[col].dtype == 'object':
            # å°è¯•è½¬æ•°å­—ï¼Œé‡åˆ°æ— æ³•è½¬æ¢çš„å˜æˆ NaN
            try_numeric = pd.to_numeric(df[col], errors='coerce')
            # å¦‚æœè½¬æ¢åå¤§éƒ¨åˆ†éƒ½ä¸æ˜¯ NaNï¼Œè¯´æ˜è¿™ä¸€åˆ—åŸæœ¬å°±æ˜¯æ•°å­—
            if try_numeric.notna().sum() > len(df) * 0.5:
                print(f"ğŸ”§ ä¿®å¤æ•°æ®ç±»å‹: {col} (Object -> Float)")
                df[col] = try_numeric
    return df


print("æ­£åœ¨æ¸…æ´—æ•°æ®ç±»å‹...")
train = clean_numeric_columns(train)
test = clean_numeric_columns(test)

# ==========================================
# 3. ç‰¹å¾å·¥ç¨‹ (V3ç‰ˆ)
# ==========================================
print("æ­£åœ¨è¿›è¡Œç‰¹å¾å·¥ç¨‹ V3...")


def engineer_features(df):
    # 1. ç‰©ç†ç‰¹å¾
    df['kinetic_energy'] = df['weight_1'] * (df['speed_mean'] ** 2)
    df['momentum'] = df['weight_1'] * df['speed_mean']

    # 2. äº¤äº’ç‰¹å¾
    df['power_demand'] = df['engine_percent_load_at_current_speed_mean'] * df['engine_speed_mean']

    # 3. ä¿®å¤åçš„ç¯å¢ƒç‰¹å¾äº¤äº’
    if 'env_wind_kph' in df.columns and 'env_sailing_value' in df.columns:
        df['wind_assist'] = df['env_wind_kph'] * df['env_sailing_value']

    return df


train = engineer_features(train)
test = engineer_features(test)

# ==========================================
# 4. ç±»åˆ«å¤„ç†
# ==========================================
drop_cols = ['ID', 'Trip_ID_first', 'Trip_ID_last', target_col]
features = [c for c in train.columns if c not in drop_cols]

# é‡æ–°å®šä¹‰ç±»åˆ«åˆ—ï¼Œè¿™æ¬¡ä¸ä¼šåŒ…å« humidity äº†
cat_cols = []
for col in features:
    # åªæœ‰çœŸæ­£çš„ ID å’Œ æ–‡æœ¬ æ‰æ˜¯ç±»åˆ«
    if train[col].dtype == 'object' or 'id' in col.lower() or 'code' in col.lower():
        # å†æ¬¡ç¡®è®¤è¿™ä¸€åˆ—ä¸æ˜¯ float
        if not pd.api.types.is_float_dtype(train[col]):
            train[col] = train[col].astype('category')
            test[col] = test[col].astype('category')
            cat_cols.append(col)

print(f"ä½¿ç”¨çš„ç‰¹å¾æ•°é‡: {len(features)}")
print(f"ğŸ”¥ çœŸæ­£çš„ç±»åˆ«ç‰¹å¾: {cat_cols}")

# ==========================================
# 5. LightGBM è®­ç»ƒ (å›å½’åŸå§‹ç›®æ ‡)
# ==========================================
folds = 5
kf = KFold(n_splits=folds, shuffle=True, random_state=2025)

# ä¸å†ä½¿ç”¨ Log å˜æ¢ï¼Œç›´æ¥é¢„æµ‹
oof_preds = np.zeros(len(train))
test_preds = np.zeros(len(test))
scores = []
feature_importance_df = pd.DataFrame()

print(f"å¼€å§‹è®­ç»ƒ LightGBM (V3 - Fixed Types)...")

for fold, (train_idx, val_idx) in enumerate(kf.split(train)):
    X_train, y_train = train[features].iloc[train_idx], train[target_col].iloc[train_idx]
    X_val, y_val = train[features].iloc[val_idx], train[target_col].iloc[val_idx]

    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'learning_rate': 0.03,
        'num_leaves': 40,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'n_jobs': -1
    }

    model = lgb.train(
        params,
        lgb.Dataset(X_train, y_train, categorical_feature=cat_cols),
        num_boost_round=5000,
        valid_sets=[lgb.Dataset(X_val, y_val)],
        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(1000)]
    )

    val_pred = model.predict(X_val)
    oof_preds[val_idx] = val_pred
    test_preds += model.predict(test[features]) / folds

    rmse = np.sqrt(mean_squared_error(y_val, val_pred))
    scores.append(rmse)
    print(f"Fold {fold + 1} RMSE: {rmse:.4f}")

    # è®°å½•é‡è¦æ€§
    fold_importance = pd.DataFrame()
    fold_importance["feature"] = features
    fold_importance["importance"] = model.feature_importance()
    fold_importance["fold"] = fold + 1
    feature_importance_df = pd.concat([feature_importance_df, fold_importance], axis=0)

# ==========================================
# 6. ç»“æœ
# ==========================================
mean_rmse = np.mean(scores)
print(f"\n========================================")
print(f"ğŸ”¥ V3ç‰ˆæœ¬ (ç±»å‹ä¿®å¤) å¹³å‡ RMSE: {mean_rmse:.4f}")
print(f"========================================")

submission = pd.DataFrame({'ID': test['ID'], target_col: test_preds})
sub_filename = f'submission_v3_fixed_rmse_{mean_rmse:.4f}.csv'
submission.to_csv(sub_filename, index=False)
print(f"æäº¤æ–‡ä»¶å·²ç”Ÿæˆ: {sub_filename}")

# ç»˜å›¾
plt.figure(figsize=(10, 8))
cols = (feature_importance_df[["feature", "importance"]]
        .groupby("feature")
        .mean()
        .sort_values(by="importance", ascending=False)[:20].index)
best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]
sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False))
plt.title('Top 20 Features (V3 - Fixed Types)')
plt.tight_layout()
plt.savefig('feature_importance_v3.png')