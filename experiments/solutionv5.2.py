import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error
import os
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# 1. å‡†å¤‡æ•°æ®
# ==========================================
base_path = '/Users/liangwenlong/study/bme/3/ml_usage/project/truck-fuel-consumption-forecast'
train_path = os.path.join(base_path, 'public_train.csv')
test_path = os.path.join(base_path, 'public_test.csv')

print("æ­£åœ¨è¯»å–æ•°æ®...")
train = pd.read_csv(train_path)
test = pd.read_csv(test_path)
target_col = 'fuel_consumption_sum'

# ==========================================
# 2. é˜²æ³„éœ²ç›®æ ‡ç¼–ç  (K-Fold Target Encoding) - æ ¸å¿ƒæŠ€æœ¯
# ==========================================
print("æ­£åœ¨æ„å»ºç‰¹å¾ (V6 - OOF Target Encoding)...")


def add_oof_target_encoding(train_df, test_df, cols, target, n_splits=5):
    """
    ä½¿ç”¨ K-Fold æ–¹å¼ç”Ÿæˆç›®æ ‡ç¼–ç ï¼Œå½»åº•æœç»æ•°æ®æ³„éœ²ã€‚
    """
    # 1. å…ˆç»™ Test é›†ç”Ÿæˆç¼–ç  (ä½¿ç”¨å…¨é‡ Train çš„å‡å€¼ï¼Œè¿™æ˜¯åˆæ³•çš„)
    for col in cols:
        # è®¡ç®—å…¨å±€å‡å€¼æ˜ å°„
        global_mean = train_df[target].mean()
        mapping = train_df.groupby(col)[target].mean()

        # æ˜ å°„åˆ° Test
        test_df[f'{col}_target_mean'] = test_df[col].map(mapping).fillna(global_mean)

        # åˆå§‹åŒ– Train çš„æ–°åˆ—
        train_df[f'{col}_target_mean'] = np.nan

    # 2. ç»™ Train é›†ç”Ÿæˆç¼–ç  (ä½¿ç”¨ Out-of-Fold æ–¹å¼)
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

    for tr_idx, val_idx in kf.split(train_df):
        # åˆ‡åˆ†æ•°æ®
        X_tr, X_val = train_df.iloc[tr_idx], train_df.iloc[val_idx]

        for col in cols:
            # åªç”¨è®­ç»ƒéƒ¨åˆ†(X_tr)æ¥è®¡ç®—å‡å€¼
            means = X_tr.groupby(col)[target].mean()

            # æ˜ å°„åˆ°éªŒè¯éƒ¨åˆ†(X_val)
            # æ³¨æ„ï¼šå¦‚æœéªŒè¯é›†é‡Œæœ‰è®­ç»ƒé›†æ²¡è§è¿‡çš„ç±»åˆ«ï¼Œå¡«å…¨å±€å‡å€¼
            train_df.loc[val_idx, f'{col}_target_mean'] = X_val[col].map(means)

    # 3. å¡«å…… Train ä¸­å¯èƒ½äº§ç”Ÿçš„ NaN (æ¯”å¦‚æŸæŠ˜é‡Œå‡ºç°äº†ç”Ÿåƒ»ç±»åˆ«)
    for col in cols:
        global_mean = train_df[target].mean()
        train_df[f'{col}_target_mean'] = train_df[f'{col}_target_mean'].fillna(global_mean)

    return train_df, test_df


# --- A. ç‰©ç†ç‰¹å¾ ---
for df in [train, test]:
    df['kinetic_energy'] = df['weight_1'] * (df['speed_mean'] ** 2)
    df['momentum'] = df['weight_1'] * df['speed_mean']
    df['power_demand'] = df['engine_percent_load_at_current_speed_mean'] * df['engine_speed_mean']

    if 'env_wind_kph' in df.columns and 'env_sailing_value' in df.columns:
        df['env_wind_kph'] = pd.to_numeric(df['env_wind_kph'], errors='coerce').fillna(0)
        df['env_sailing_value'] = pd.to_numeric(df['env_sailing_value'], errors='coerce').fillna(0)
        df['wind_assist'] = df['env_wind_kph'] * df['env_sailing_value']

# --- B. æ‰§è¡Œé˜²æ³„éœ²ç¼–ç  ---
# è¿™äº›æ˜¯é«˜ç»´ç±»åˆ«ç‰¹å¾ï¼Œæœ€å®¹æ˜“æ³„éœ²
te_cols = ['driver_name_and_id', 'vehicle_type', 'route_id', 'vehicle_motortype', 'deviceuniquecode']
existing_te_cols = [c for c in te_cols if c in train.columns]

print(f"ğŸ”¥ æ­£åœ¨æ‰§è¡Œ OOF Target Encoding (é˜²æ­¢æ³„éœ²): {existing_te_cols}")
# æ³¨æ„ï¼šè¿™ä¸€æ­¥ä¼šæ¯”è¾ƒæ…¢ï¼Œå› ä¸ºè¦åœ¨å†…éƒ¨è·‘ä¸€é 5æŠ˜äº¤å‰
train, test = add_oof_target_encoding(train, test, existing_te_cols, target_col)

# ==========================================
# 3. ç­›é€‰ç‰¹å¾
# ==========================================
drop_cols = ['ID', 'Trip_ID_first', 'Trip_ID_last', target_col]
features = [c for c in train.columns if c not in drop_cols]

# å¼ºåˆ¶æŒ‡å®šç±»åˆ« (è¾…åŠ© LightGBM)
cat_cols = []
for col in features:
    is_id_col = any(x in col.lower() for x in ['type', 'id', 'code', 'name'])
    is_not_te = '_target_mean' not in col
    if (train[col].dtype == 'object' or is_id_col) and is_not_te:
        train[col] = train[col].astype('category')
        test[col] = test[col].astype('category')
        cat_cols.append(col)

print(f"æœ€ç»ˆç‰¹å¾æ•°: {len(features)}")

# ==========================================
# 4. è®­ç»ƒ (V6 - MAE + Robust CV)
# ==========================================
folds = 5
kf = KFold(n_splits=folds, shuffle=True, random_state=42)

oof_preds = np.zeros(len(train))
test_preds = np.zeros(len(test))
scores = []

print(f"å¼€å§‹è®­ç»ƒ LightGBM (V6 - Robust)...")

for fold, (train_idx, val_idx) in enumerate(kf.split(train)):
    X_train, y_train = train[features].iloc[train_idx], train[target_col].iloc[train_idx]
    X_val, y_val = train[features].iloc[val_idx], train[target_col].iloc[val_idx]

    params = {
        'objective': 'mae',
        'metric': 'mae',
        'boosting_type': 'gbdt',
        'learning_rate': 0.03,
        'num_leaves': 64,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'n_jobs': -1
    }

    model = lgb.train(
        params,
        lgb.Dataset(X_train, y_train, categorical_feature=cat_cols),
        num_boost_round=10000,
        valid_sets=[lgb.Dataset(X_val, y_val)],
        callbacks=[lgb.early_stopping(300), lgb.log_evaluation(1000)]
    )

    val_pred = model.predict(X_val)
    oof_preds[val_idx] = val_pred
    test_preds += model.predict(test[features]) / folds

    score = mean_absolute_error(y_val, val_pred)
    scores.append(score)
    print(f"Fold {fold + 1} MAE: {score:.4f}")

# ==========================================
# 5. ç»“æœ
# ==========================================
mean_mae = np.mean(scores)
print(f"\n========================================")
print(f"ğŸ”¥ V6ç‰ˆæœ¬ (æ— æ³„éœ²) å¹³å‡ MAE: {mean_mae:.4f}")
print(f"========================================")

submission = pd.DataFrame({'ID': test['ID'], target_col: test_preds})
sub_filename = f'submission_v6_robust_mae_{mean_mae:.4f}.csv'
submission.to_csv(sub_filename, index=False)
print(f"âœ… æœ€ç¨³å¥çš„æäº¤æ–‡ä»¶å·²ç”Ÿæˆ: {sub_filename}")