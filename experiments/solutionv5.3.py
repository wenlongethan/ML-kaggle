import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error
import os
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# 1. å‡†å¤‡æ•°æ®
# ==========================================
base_path = '/Users/liangwenlong/study/bme/3/ml_usage/project/truck-fuel-consumption-forecast'
train_path = os.path.join(base_path, 'public_train.csv')
test_path = os.path.join(base_path, 'public_test.csv')

print("æ­£åœ¨è¯»å–æ•°æ®...")
train = pd.read_csv(train_path)
test = pd.read_csv(test_path)
target_col = 'fuel_consumption_sum'
test_ids = test['ID']

# ==========================================
# 2. ç‰¹å¾å·¥ç¨‹ (ä¿æŒ V5.1 çš„ç²¾å)
# ==========================================
print("æ­£åœ¨æ„å»ºç‰¹å¾ (V5.3 - Regularized)...")


def add_oof_target_encoding(train_df, test_df, cols, target, n_splits=5):
    # 1. Test Encoding (ç”¨å…¨é‡ Train)
    for col in cols:
        global_mean = train_df[target].mean()
        mapping = train_df.groupby(col)[target].mean()
        test_df[f'{col}_te'] = test_df[col].map(mapping).fillna(global_mean)
        train_df[f'{col}_te'] = np.nan

    # 2. Train Encoding (OOF)
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    for tr_idx, val_idx in kf.split(train_df):
        X_tr, X_val = train_df.iloc[tr_idx], train_df.iloc[val_idx]
        for col in cols:
            means = X_tr.groupby(col)[target].mean()
            train_df.loc[val_idx, f'{col}_te'] = X_val[col].map(means)

    # 3. Fill NaNs
    for col in cols:
        global_mean = train_df[target].mean()
        train_df[f'{col}_te'] = train_df[f'{col}_te'].fillna(global_mean)

    return train_df, test_df


# --- ç‰©ç†ç‰¹å¾ ---
for df in [train, test]:
    df['kinetic_energy'] = df['weight_1'] * (df['speed_mean'] ** 2)
    df['momentum'] = df['weight_1'] * df['speed_mean']
    df['power_demand'] = df['engine_percent_load_at_current_speed_mean'] * df['engine_speed_mean']

    if 'env_wind_kph' in df.columns and 'env_sailing_value' in df.columns:
        df['env_wind_kph'] = pd.to_numeric(df['env_wind_kph'], errors='coerce').fillna(0)
        df['env_sailing_value'] = pd.to_numeric(df['env_sailing_value'], errors='coerce').fillna(0)
        df['wind_assist'] = df['env_wind_kph'] * df['env_sailing_value']

# --- OOF Target Encoding ---
te_cols = ['driver_name_and_id', 'vehicle_type', 'route_id', 'vehicle_motortype', 'deviceuniquecode']
existing_te_cols = [c for c in te_cols if c in train.columns]
train, test = add_oof_target_encoding(train, test, existing_te_cols, target_col)

# ==========================================
# 3. ç­›é€‰ä¸æ¸…æ´—
# ==========================================
drop_cols = ['ID', 'Trip_ID_first', 'Trip_ID_last', target_col]
# ä¸¢å¼ƒåŸå§‹ ID åˆ—ï¼Œåªç”¨ TE ç‰¹å¾ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
drop_cols += existing_te_cols

features = [c for c in train.columns if c not in drop_cols]

# å‰©ä½™çš„ç±»åˆ«è½¬ category
cat_cols = []
for col in features:
    if train[col].dtype == 'object':
        train[col] = train[col].astype('category')
        test[col] = test[col].astype('category')
        cat_cols.append(col)

print(f"ä½¿ç”¨çš„ç‰¹å¾æ•°: {len(features)}")

# ==========================================
# 4. è®­ç»ƒ (V5.3 - å¼ºæ­£åˆ™åŒ–ç‰ˆ)
# ==========================================
folds = 5
kf = KFold(n_splits=folds, shuffle=True, random_state=42)

oof_preds = np.zeros(len(train))
test_preds = np.zeros(len(test))
scores = []

print(f"å¼€å§‹è®­ç»ƒ LightGBM (V5.3 Regularized)...")

for fold, (train_idx, val_idx) in enumerate(kf.split(train)):
    X_train, y_train = train[features].iloc[train_idx], train[target_col].iloc[train_idx]
    X_val, y_val = train[features].iloc[val_idx], train[target_col].iloc[val_idx]

    # !!! å…³é”®æ”¹åŠ¨ !!!
    params = {
        'objective': 'mae',
        'metric': 'mae',
        'boosting_type': 'gbdt',
        'learning_rate': 0.03,
        'num_leaves': 40,  # ä» 64 é™åˆ° 40ï¼Œé™ä½å¤æ‚åº¦
        'feature_fraction': 0.7,  # ä» 0.9 é™åˆ° 0.7ï¼Œå¢åŠ éšæœºæ€§
        'bagging_fraction': 0.7,
        'bagging_freq': 5,
        'lambda_l1': 1.0,  # [æ–°å¢] L1 æ­£åˆ™åŒ–
        'lambda_l2': 1.0,  # [æ–°å¢] L2 æ­£åˆ™åŒ–
        'min_child_samples': 50,  # [æ–°å¢] é¿å…å¶å­å¤ªå°
        'verbose': -1,
        'n_jobs': -1
    }

    model = lgb.train(
        params,
        lgb.Dataset(X_train, y_train, categorical_feature=cat_cols),
        num_boost_round=10000,
        valid_sets=[lgb.Dataset(X_val, y_val)],
        callbacks=[lgb.early_stopping(300), lgb.log_evaluation(1000)]
    )

    val_pred = model.predict(X_val)
    oof_preds[val_idx] = val_pred
    test_preds += model.predict(test[features]) / folds

    score = mean_absolute_error(y_val, val_pred)
    scores.append(score)
    print(f"Fold {fold + 1} MAE: {score:.4f}")

# ==========================================
# 5. ç»“æœ
# ==========================================
mean_mae = np.mean(scores)
print(f"\n========================================")
print(f"ğŸ”¥ V5.3 (æ­£åˆ™åŒ–ç‰ˆ) å¹³å‡ MAE: {mean_mae:.4f}")
print(f"========================================")

submission = pd.DataFrame({'ID': test_ids, target_col: test_preds})
sub_filename = f'submission_v5_3_reg_mae_{mean_mae:.4f}.csv'
submission.to_csv(sub_filename, index=False)
print(f"æäº¤æ–‡ä»¶å·²ç”Ÿæˆ: {sub_filename}")