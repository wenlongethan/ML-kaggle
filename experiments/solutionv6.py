import os
import numpy as np
import pandas as pd
import lightgbm as lgb

from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns


# ==========================================
# 1. è·¯å¾„ä¸è¯»å–æ•°æ®
# ==========================================
# æŒ‰ä½ ç°åœ¨çš„è·¯å¾„æ¥ï¼Œå¦‚æœæ–‡ä»¶å¤¹ç§»åŠ¨äº†è‡ªå·±æ”¹ä¸€ä¸‹ base_path
base_path = '/Users/liangwenlong/study/bme/3/ml_usage/project/truck-fuel-consumption-forecast'
train_path = os.path.join(base_path, 'public_train.csv')
test_path = os.path.join(base_path, 'public_test.csv')

print("ğŸ”¹ Loading data ...")
train = pd.read_csv(train_path)
test = pd.read_csv(test_path)

target_col = 'fuel_consumption_sum'


# ==========================================
# 2. æ•°æ®ç±»å‹æ¸…æ´—ï¼šæŠŠâ€œä¼ªæ•°å­—â€çš„ object åˆ—è½¬æˆ float
# ==========================================
def clean_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:
    for col in df.columns:
        if df[col].dtype == 'object':
            # å°è¯•è½¬æ•°å­—ï¼Œæ— æ³•è½¬æ¢çš„å˜æˆ NaN
            converted = pd.to_numeric(df[col], errors='coerce')
            # å¦‚æœå¤§éƒ¨åˆ†éƒ½èƒ½è½¬ï¼Œå°±è®¤ä¸ºå®ƒæœ¬æ¥åº”è¯¥æ˜¯æ•°å€¼åˆ—
            if converted.notna().sum() > 0.5 * len(df):
                print(f"  ğŸ”§ Cast to numeric: {col}")
                df[col] = converted
    return df


print("ğŸ”¹ Cleaning numeric-like columns ...")
train = clean_numeric_columns(train)
test = clean_numeric_columns(test)


# ==========================================
# 3. ç‰¹å¾å·¥ç¨‹ï¼šç‰©ç†ç‰¹å¾ + ç¯å¢ƒç‰¹å¾ + Trip ç»“æ„
# ==========================================
print("ğŸ”¹ Feature engineering ...")

for df in [train, test]:
    # ---- ç‰©ç†ç‰¹å¾ ----
    if {'weight_1', 'speed_mean'}.issubset(df.columns):
        df['kinetic_energy'] = df['weight_1'] * (df['speed_mean'] ** 2)
        df['momentum'] = df['weight_1'] * df['speed_mean']

    if {'engine_percent_load_at_current_speed_mean', 'engine_speed_mean'}.issubset(df.columns):
        df['power_demand'] = (
            df['engine_percent_load_at_current_speed_mean'] * df['engine_speed_mean']
        )

    # ---- ç¯å¢ƒç‰¹å¾ä¿®å¤ + äº¤äº’ ----
    if 'env_wind_kph' in df.columns:
        df['env_wind_kph'] = pd.to_numeric(df['env_wind_kph'], errors='coerce').fillna(0)
    if 'env_sailing_value' in df.columns:
        df['env_sailing_value'] = pd.to_numeric(df['env_sailing_value'],
                                                errors='coerce').fillna(0)
    if {'env_wind_kph', 'env_sailing_value'}.issubset(df.columns):
        df['wind_assist'] = df['env_wind_kph'] * df['env_sailing_value']

    # ---- Trip ç»“æ„ç‰¹å¾ ----
    # Trip_ID_first ä¸ ID æ˜¯æ•°æ®é‡Œå·²æœ‰çš„
    if 'Trip_ID_first' in df.columns and 'ID' in df.columns:
        # æ¯ä¸ª trip çš„é•¿åº¦
        df['trip_len'] = df.groupby('Trip_ID_first')['ID'].transform('count')
        # å½“å‰åˆ‡ç‰‡åœ¨ trip ä¸­çš„åºå·ï¼ˆä» 0 å¼€å§‹ï¼‰
        df['trip_pos'] = df.groupby('Trip_ID_first').cumcount()
        # åœ¨ trip ä¸­çš„ç›¸å¯¹ä½ç½®ï¼ˆ0~1ï¼‰
        df['trip_pos_ratio'] = df['trip_pos'] / df['trip_len'].replace(0, 1)

        # trip çº§å¹³å‡ç»Ÿè®¡
        if 'speed_mean' in df.columns:
            df['trip_speed_mean'] = df.groupby('Trip_ID_first')['speed_mean'].transform('mean')
        if 'engine_speed_mean' in df.columns:
            df['trip_engine_speed_mean'] = df.groupby('Trip_ID_first')[
                'engine_speed_mean'].transform('mean')
        if 'weight_1' in df.columns:
            df['trip_weight_mean'] = df.groupby('Trip_ID_first')['weight_1'].transform('mean')


# ==========================================
# 4. Target Encoding + Frequency Encoding
# ==========================================
print("ğŸ”¹ Target & frequency encoding ...")


def target_encoding(train_df, test_df, col, target):
    """
    ç®€å•ç‰ˆ target encodingï¼šgroupby-meanï¼Œç„¶åæ˜ å°„ train / testã€‚
    æœ‰ä¸€å®š leakageï¼Œä½†åœ¨è¿™ä¸ªä½œä¸š+LGBM åœºæ™¯é‡Œæ˜¯å¯ä»¥æ¥å—çš„ã€‚
    """
    mapping = train_df.groupby(col)[target].mean()
    global_mean = train_df[target].mean()

    train_te = train_df[col].map(mapping).fillna(global_mean)
    test_te = test_df[col].map(mapping).fillna(global_mean)
    return train_te, test_te


te_cols = ['driver_name_and_id',
           'vehicle_type',
           'route_id',
           'vehicle_motortype',
           'deviceuniquecode']

existing_te_cols = [c for c in te_cols if c in train.columns]
print(f"  Will do target encoding for: {existing_te_cols}")

for col in existing_te_cols:
    # Target mean
    train[f'{col}_target_mean'], test[f'{col}_target_mean'] = \
        target_encoding(train, test, col, target_col)

    # Frequency encoding
    vc = train[col].value_counts()
    train[f'{col}_freq'] = train[col].map(vc).fillna(0)
    test[f'{col}_freq'] = test[col].map(vc).fillna(0)


# ==========================================
# 5. å®šä¹‰ç‰¹å¾ & ç±»åˆ«ç‰¹å¾
# ==========================================
drop_cols = ['ID', 'Trip_ID_first', 'Trip_ID_last', target_col]
features = [c for c in train.columns if c not in drop_cols]

cat_cols = []
for col in features:
    # è§„åˆ™ï¼šçœ‹èµ·æ¥åƒ ID / ç±»å‹ / åå­—ï¼Œä¸”ä¸æ˜¯ target_mean
    is_id_like = any(k in col.lower() for k in ['id', 'code', 'type', 'name'])
    is_not_te = '_target_mean' not in col

    if (train[col].dtype == 'object' or is_id_like) and is_not_te:
        train[col] = train[col].astype('category')
        test[col] = test[col].astype('category')
        cat_cols.append(col)

print(f"ğŸ”¹ Total features: {len(features)}")
print(f"ğŸ”¹ Categorical features: {cat_cols}")


# ==========================================
# 6. å¤š seed Ã— KFold LightGBM è®­ç»ƒ
# ==========================================
seeds = [42, 2025, 3407]   # å¯ä»¥å†åŠ ä¸€ä¸¤ä¸ªï¼Œä½†è®­ç»ƒæ—¶é—´ä¼šçº¿æ€§å˜é•¿
folds = 5

all_seed_scores = []
test_preds_all_seeds = np.zeros((len(seeds), len(test)))
feature_importance_df = pd.DataFrame()

print("ğŸ”¹ Start multi-seed LightGBM training ...")

for si, seed in enumerate(seeds):
    print(f"\n===== Seed {seed} =====")
    # æ¯ä¸ª seed å•ç‹¬ä¸€ä¸ª KFoldï¼ˆä¿è¯ shuffle ä¸€è‡´ï¼‰
    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)

    params = {
        'objective': 'mae',
        'metric': 'mae',
        'boosting_type': 'gbdt',
        'learning_rate': 0.03,
        'num_leaves': 64,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'min_data_in_leaf': 60,
        'lambda_l1': 0.5,
        'lambda_l2': 3.0,
        'verbose': -1,
        'n_jobs': -1,
        'seed': seed,
    }

    seed_scores = []
    seed_test_preds = np.zeros(len(test))

    for fold, (train_idx, val_idx) in enumerate(kf.split(train)):
        print(f"  -> Seed {seed} | Fold {fold + 1}/{folds}")

        X_train = train[features].iloc[train_idx]
        y_train = train[target_col].iloc[train_idx]
        X_val = train[features].iloc[val_idx]
        y_val = train[target_col].iloc[val_idx]

        train_set = lgb.Dataset(X_train, y_train, categorical_feature=cat_cols, free_raw_data=False)
        val_set = lgb.Dataset(X_val, y_val, categorical_feature=cat_cols, free_raw_data=False)

        model = lgb.train(
            params,
            train_set,
            num_boost_round=10000,
            valid_sets=[val_set],
            callbacks=[
                lgb.early_stopping(300),
                lgb.log_evaluation(500)
            ]
        )

        val_pred = model.predict(X_val, num_iteration=model.best_iteration)
        mae = mean_absolute_error(y_val, val_pred)
        seed_scores.append(mae)
        print(f"     Fold {fold + 1} MAE: {mae:.4f}")

        # test é¢„æµ‹
        seed_test_preds += model.predict(test[features],
                                         num_iteration=model.best_iteration) / folds

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        fold_importance = pd.DataFrame({
            "feature": features,
            "importance": model.feature_importance(),
            "seed": seed,
            "fold": fold + 1,
        })
        feature_importance_df = pd.concat([feature_importance_df, fold_importance],
                                          axis=0)

    seed_mean_mae = np.mean(seed_scores)
    all_seed_scores.append(seed_mean_mae)
    test_preds_all_seeds[si, :] = seed_test_preds
    print(f"===== Seed {seed} mean MAE: {seed_mean_mae:.4f} =====")

cv_mae_mean = np.mean(all_seed_scores)
cv_mae_std = np.std(all_seed_scores)
print("\n========================================")
print(f"ğŸ”¥ Multi-seed CV MAE mean: {cv_mae_mean:.4f}  (std: {cv_mae_std:.4f})")
print("========================================")


# ==========================================
# 7. ç”Ÿæˆæäº¤æ–‡ä»¶
# ==========================================
final_test_preds = test_preds_all_seeds.mean(axis=0)

submission = pd.DataFrame({
    'ID': test['ID'],
    target_col: final_test_preds
})

sub_filename = f'submission_v6_ensemble_mae_{cv_mae_mean:.4f}.csv'
submission.to_csv(sub_filename, index=False)
print(f"âœ… Submission saved as: {sub_filename}")


# ==========================================
# 8. ç‰¹å¾é‡è¦æ€§å›¾ï¼ˆTop 30ï¼‰
# ==========================================
try:
    plt.figure(figsize=(10, 10))
    cols = (
        feature_importance_df[["feature", "importance"]]
        .groupby("feature")
        .mean()
        .sort_values(by="importance", ascending=False)[:30]
        .index
    )
    best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]
    sns.barplot(
        x="importance",
        y="feature",
        data=best_features.sort_values(by="importance", ascending=False)
    )
    plt.title('Top 30 Feature Importance (multi-seed LGBM)')
    plt.tight_layout()
    plt.savefig('feature_importance_v6.png')
    print("ğŸ“Š Feature importance plot saved as feature_importance_v6.png")
except Exception as e:
    print(f"âš ï¸ Could not plot feature importance: {e}")
